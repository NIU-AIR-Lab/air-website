---
title: 'Intrinsic Validation: Analogy Accuracy and Transparency'
author: Alex Wold
date: '2021-06-13'
slug: intrinsic-validation-analogy-accuracy-and-transparency
categories:
  - Intrinsic Validation
  - Analogies
tags:
  - Accuracy
  - Transparency
subtitle: ''
summary: ''
authors: []
lastmod: '2021-06-13T02:33:15-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---
Comparing model similarity scores with human similarity scores offers valuable insights concerning the performances of our embeddings, but it isn't the only way to screen for the best word representations. Alike in nature, analogies provide an alternative measure for gauging the quality of our vocabulary tensors, appearing in the form: **a** is to **b** as **c** is to **d**. For example,  
<center>`amazing` is to `amazingly` as `apparent` is to `apparently`</center>  
Here, we note a difference in the syntactic axis from *amazing* to *amazingly* and from *apparent* to *apparently*. If we wanted to test for differences across the semantic dimension, we could use:  
<center>`man` is to `king` as `woman` is to `queen`</center>
These examples reveal the fundamental purpose of our analogy pursuits, to test the embedding of its understanding regarding the changes and associations in linguistic features. We can provide the computer with the first three terms in the 4-tuple - **a**, **b**, and **c** - and ask it to assign its own value for **d**. Luckily, the python module **Gensim** supplies an `evaluate_word_analogies()` function that utilizes cosine similarity to perform that operation for us. The embedding adds terms **b** and **c**, subtracts **a**, and then finds the closest match within the vocabulary to give its final guess of **d**, which we then compare with the actual, listed answer.  

If even one of the terms doesn't appear within the embedding vocabulary, we tag the analogy as OOV (out of vocabulary) and throw it out so that it doesn't erroneously affect the output. In total, the accuracy percentage includes only the 4-tuples where each word appears in the vector argot, and where the model estimated the **d** term correctly the *first time*. Unfortunately, analogy evaluations tend to score much lower than similarity comparisons (usually around a 30% accurate for high scores), changing drastically depending on the OOV ratio.

Gensim's `evaluate_word_analogies()` function delivers a simple metric perfect for swift, broad judgment, but it doesn't necessarily provide an embedding-by-embedding breakdown for each analogy. The `sections` member of the response offers a close estimation, but it fails to build a side-by-side comparison of the correct vs. the estimated **d** terms. Moreover, the function never lists any other alternative model solutions, leaving the product somewhat opaque in regards to how well our model can actually complete an analogy. Using only the default function as-is, we don't really know how well the embedding "understands" the given text. As a result, we implemented a function of our own that incorporates this functionality. We borrowed the logic of Gensim's method and included a more verbose set of reporting tools that records the first five guesses of our embedding and their respective cosine similarities:

```{r, echo = FALSE}
library(lemon)
library(knitr)

knitr::opts_chunk$set(fig.path = "static")
knit_print.data.frame <- lemon_print
```


```{r, echo = FALSE, render = lemon_print, caption = "Verbose Output (truncated at only 1 guess)"}
verbframe <- data.frame(A = "tangible",	B = "depreciation", C = "intangible",	D = "amortization",
                        cosim1 = 0.633,
                        guess1 = "depreciated")

verbframe
```



With a clearer idea of where our embeddings make mistakes, we can use the output to refine our training methods for a better set of word representations in the future. 
