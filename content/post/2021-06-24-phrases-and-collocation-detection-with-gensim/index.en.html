---
title: Phrases and Collocation Detection with Gensim
author: Alex Wold
date: '2021-06-24'
slug: phrases-and-collocation-detection-with-gensim
categories:
  - N-Grams
tags:
  - Gensim
  - Collocation
  - Phrases
  - N-Grams
subtitle: ''
summary: ''
authors: []
lastmod: '2021-06-24T01:17:10-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<p>So far, we’ve used a set-list of replacement rules to incorporate n-grams within our tax-embeddings, but the Gensim Phrases library allows us to train a customized tokenizer to identify multiple-word terms specific to the IRS corpora using collocation detection. This pushes our model to capture expressions beyond the default unigram level and our own preservation strategies.</p>
<p>It’s easier to work directly in python when using the Gensim functionalities. Therefore, we’ll start with the following libraries and load a our example corpus for training and testing.</p>
<pre class="r"><code># load packages
library(reticulate)
library(stringr)
library(tokenizers)

# load the instructions (as object instext)
textpath &lt;- &quot;/home/alex/Projects/taxembed/data/corpora/irs_corpi/instructions.rda&quot;
load(textpath)</code></pre>
<p><em>instructions.rda</em> refers to a vector of strings, where each string is the ripped text from an individual IRS instruction posting. We don’t need all of the data for a simple trial run, so we’ll just use the first seven instruction texts (six for training, one for testing).</p>
<p>Unfortunately, the example corpus isn’t very clean; regex expressions, like <code>\n</code> and <code>\t</code>, riddle the text with symbols that don’t belong in the n-gram vocabulary. Luckily, the <code>stringr</code> package provides a few removal functions that can target these expressions and eliminate duplicate spaces. Afterwords, we can apply a sentence and word tokenizer to split the data into the form of a list, where each entry consists of a character vector of the separated words. We then convert the objects to their python equivalents, where we can access them by <code>r.words</code> for the training set and <code>r.twords</code> for the testing set.</p>
<pre class="r"><code># create example corpus
excor &lt;- paste(instext[1], instext[2], instext[3], instext[4], instext[5], instext[6])
testcor &lt;- instext[7]

# replace regex characters and duplicate spaces
excor &lt;- str_replace_all(excor, pattern = &quot;(\\n|\\t)+&quot;, replacement = &quot; &quot;)
excor &lt;- str_replace_all(excor, pattern = &quot;\\s{2,}&quot;, replacement = &quot; &quot;)
testcor &lt;- str_replace_all(testcor, pattern = &quot;(\\n|\\t)+&quot;, replacement = &quot; &quot;)
testcor &lt;- str_replace_all(testcor, pattern = &quot;\\s{2,}&quot;, replacement = &quot; &quot;)

# tokenize the string into sentences
sentences &lt;- unlist(tokenize_sentences(excor))
testsents &lt;- unlist(tokenize_sentences(testcor))

# tokenize the sentences into words
words &lt;- lapply(sentences, tokenize_words)
twords &lt;- lapply(testsents, tokenize_words)

# unlist the secondary lists for proper form
for (i in 1:length(words)) {
  words[[i]] &lt;- unlist(words[i])
}

for (i in 1:length(twords)) {
  twords[[i]] &lt;- unlist(twords[i])
}

# convert the r object to a python object (# access with r.words in python chunk)
r_to_py(words)
r_to_py(twords)</code></pre>
<pre><code>## List (3206 items)
## List (299 items)</code></pre>
<p>It’s easier to work directly in python when using the Gensim library, so we’ll need to switch to an anaconda environment and load a few modules. Then, we train the first bigram and the succeeding trigram (by feeding the bigram through the first tgram).</p>
<pre class="python"><code>from gensim.models.phrases import Phrases, Phraser

# first train a bigram
bigram = Phrases(r.words)
trigram = Phrases(bigram[r.words])</code></pre>
<p>Using a portion of the testing data, we feed in a sentence and let the trigram connect the most likely bigrams together.</p>
<pre class="python"><code># print the original sentence
print(&quot;Original Sentence:&quot;)</code></pre>
<pre><code>## Original Sentence:</code></pre>
<pre class="python"><code>print(r.twords[86])

# run the sentence through the trigram model</code></pre>
<pre><code>## [&#39;this&#39;, &#39;rule&#39;, &#39;does&#39;, &#39;not&#39;, &#39;apply&#39;, &#39;to&#39;, &#39;a&#39;, &#39;transfer&#39;, &#39;to&#39;, &#39;an&#39;, &#39;individual&#39;, &#39;who&#39;, &#39;is&#39;, &#39;not&#39;, &#39;a&#39;, &#39;lineal&#39;, &#39;descendant&#39;, &#39;of&#39;, &#39;the&#39;, &#39;transferor&#39;, &#39;if&#39;, &#39;the&#39;, &#39;transferor&#39;, &#39;has&#39;, &#39;any&#39;, &#39;living&#39;, &#39;lineal&#39;, &#39;descendants&#39;]</code></pre>
<pre class="python"><code>print(f&quot;\n\nPhrased Sentence:&quot;)</code></pre>
<pre><code>## 
## 
## Phrased Sentence:</code></pre>
<pre class="python"><code>print(trigram[bigram[r.twords[86]]])</code></pre>
<pre><code>## [&#39;this_rule&#39;, &#39;does_not_apply&#39;, &#39;to&#39;, &#39;a&#39;, &#39;transfer&#39;, &#39;to&#39;, &#39;an_individual&#39;, &#39;who&#39;, &#39;is&#39;, &#39;not&#39;, &#39;a&#39;, &#39;lineal_descendant&#39;, &#39;of&#39;, &#39;the&#39;, &#39;transferor&#39;, &#39;if&#39;, &#39;the&#39;, &#39;transferor&#39;, &#39;has&#39;, &#39;any&#39;, &#39;living&#39;, &#39;lineal_descendants&#39;]</code></pre>
<p>Note: we must run the sentence through the bigram layer first, and then through the subsequent, higher-level phrasers thereafter. Some vocabulary terms look like 4-grams, but they’re just two bigrams attached in the trigram process.</p>
<p>In the training stage, we can further adjust the phraser using the tuning parameters available with the <code>Phrases()</code> function. <em>min_count</em> will ignore all words and bigrams with total collected counts lower than the provided value, and <em>threshold</em> will determine the sensitivity of the model when detecting n-grams. The parser will accept Phrase A followed by Phrase B as a bigram only if it scores higher than this threshold value, which in turn relies on the <em>scoring</em> parameter. Therefore, higher threshold values result in a lower amount of recognized bigrams, see the example below.</p>
<pre class="python"><code>from gensim.models.phrases import Phrases, Phraser

# first train a bigram
bigram2 = Phrases(r.words, threshold = 50)
trigram2 = Phrases(bigram[r.words], threshold = 50)

# print the original sentence
print(&quot;Original Sentence:&quot;)</code></pre>
<pre><code>## Original Sentence:</code></pre>
<pre class="python"><code>print(r.twords[86])

# run the sentence through the trigram model</code></pre>
<pre><code>## [&#39;this&#39;, &#39;rule&#39;, &#39;does&#39;, &#39;not&#39;, &#39;apply&#39;, &#39;to&#39;, &#39;a&#39;, &#39;transfer&#39;, &#39;to&#39;, &#39;an&#39;, &#39;individual&#39;, &#39;who&#39;, &#39;is&#39;, &#39;not&#39;, &#39;a&#39;, &#39;lineal&#39;, &#39;descendant&#39;, &#39;of&#39;, &#39;the&#39;, &#39;transferor&#39;, &#39;if&#39;, &#39;the&#39;, &#39;transferor&#39;, &#39;has&#39;, &#39;any&#39;, &#39;living&#39;, &#39;lineal&#39;, &#39;descendants&#39;]</code></pre>
<pre class="python"><code>print(f&quot;\n\nPhrased Sentence:&quot;)</code></pre>
<pre><code>## 
## 
## Phrased Sentence:</code></pre>
<pre class="python"><code>print(trigram2[bigram2[r.twords[86]]])</code></pre>
<pre><code>## [&#39;this&#39;, &#39;rule&#39;, &#39;does_not&#39;, &#39;apply&#39;, &#39;to&#39;, &#39;a&#39;, &#39;transfer&#39;, &#39;to&#39;, &#39;an&#39;, &#39;individual&#39;, &#39;who&#39;, &#39;is&#39;, &#39;not&#39;, &#39;a&#39;, &#39;lineal_descendant&#39;, &#39;of&#39;, &#39;the&#39;, &#39;transferor&#39;, &#39;if&#39;, &#39;the&#39;, &#39;transferor&#39;, &#39;has&#39;, &#39;any&#39;, &#39;living&#39;, &#39;lineal_descendants&#39;]</code></pre>
<p>Note: “this” and “rule” were not concatenated as “this_rule” with the higher threshold. A higher value means fewer phrases.</p>
<p>Again, this threshold depends on the scoring function used to specify how the model evaluates potential phrases. Gensim allows the user to change the default function to npmi_scorer(), or they can provide their own method.</p>
<p><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>
<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>
<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>
<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://radimrehurek.com/gensim/models/phrases.html" class="uri">https://radimrehurek.com/gensim/models/phrases.html</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p><a href="https://radimrehurek.com/gensim_3.8.3/models/phrases.html" class="uri">https://radimrehurek.com/gensim_3.8.3/models/phrases.html</a><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p><a href="https://tedboy.github.io/nlps/_modules/gensim/models/phrases.html" class="uri">https://tedboy.github.io/nlps/_modules/gensim/models/phrases.html</a><a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p><a href="https://cran.r-project.org/web/packages/tokenizers/vignettes/introduction-to-tokenizers.html" class="uri">https://cran.r-project.org/web/packages/tokenizers/vignettes/introduction-to-tokenizers.html</a><a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
