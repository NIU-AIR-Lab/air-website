---
title: Phrases and Collocation Detection with Gensim
author: Alex Wold
date: '2021-06-24'
slug: phrases-and-collocation-detection-with-gensim
categories:
  - N-Grams
tags:
  - Gensim
  - Collocation
  - Phrases
  - N-Grams
subtitle: ''
summary: ''
authors: []
lastmod: '2021-06-24T01:17:10-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<p>So far, we’ve used a set-list of replacement rules to incorporate n-grams within our tax-embeddings, but the Gensim Phrases library allows us to train a customized tokenizer to identify multiple-word terms specific to the IRS corpora using collocation detection. This pushes our model to capture expressions beyond the default unigram level and our own preservation strategies.</p>
<p>It’s easier to work directly in python when using the Gensim functionalities. Therefore, we’ll start with the following libraries and load a our example corpus for training and testing.</p>
<pre class="r"><code># load packages
library(reticulate)
library(stringr)
library(tokenizers)

# load the instructions (as object instext)
textpath &lt;- &quot;/home/alex/Projects/taxembed/data/corpora/irs_corpi/instructions.rda&quot;
load(textpath)</code></pre>
<p><em>instructions.rda</em> refers to a vector of strings, where each string is the ripped text from an individual IRS instruction posting. We don’t need all of the data for a simple trial run, so we’ll just use the first four instruction texts (three for training, one for testing).</p>
<p>Unfortunately, the example corpus isn’t very clean; regex expressions, like <code>\n</code> and <code>\t</code>, riddle the text with symbols that don’t belong in the n-gram vocabulary. Luckily, the <code>stringr</code> package provides a few removal functions that can target these expressions and eliminate duplicate spaces. Afterwords, we can apply a sentence and word tokenizer to split the data into the form of a list, where each entry consists of a character vector of the separated words. We then convert the objects to their python equivalents, where we can access them by <code>r.words</code> for the training set and <code>r.twords</code> for the testing set.</p>
<pre class="r"><code># create example corpus
excor &lt;- paste(instext[1], instext[2], instext[3], instext[4], instext[5], instext[6])
testcor &lt;- instext[7]

# replace regex characters and duplicate spaces
excor &lt;- str_replace_all(excor, pattern = &quot;(\\n|\\t)+&quot;, replacement = &quot; &quot;)
excor &lt;- str_replace_all(excor, pattern = &quot;\\s{2,}&quot;, replacement = &quot; &quot;)
testcor &lt;- str_replace_all(testcor, pattern = &quot;(\\n|\\t)+&quot;, replacement = &quot; &quot;)
testcor &lt;- str_replace_all(testcor, pattern = &quot;\\s{2,}&quot;, replacement = &quot; &quot;)

# tokenize the string into sentences
sentences &lt;- unlist(tokenize_sentences(excor))
testsents &lt;- unlist(tokenize_sentences(testcor))

# tokenize the sentences into words
words &lt;- lapply(sentences, tokenize_words)
twords &lt;- lapply(testsents, tokenize_words)

# unlist the secondary lists for proper form
for (i in 1:length(words)) {
  words[[i]] &lt;- unlist(words[i])
}

for (i in 1:length(twords)) {
  twords[[i]] &lt;- unlist(twords[i])
}

# convert the r object to a python object (# access with r.words in python chunk)
r_to_py(words)
r_to_py(twords)</code></pre>
<pre><code>## List (3206 items)
## List (299 items)</code></pre>
<p>It’s easier to work directly in python when using the Gensim library, so we’ll need to switch to an anaconda environment and load a few modules. Then, we train the first bigram and the succeeding trigram (by feeding the bigram through the first tgram).</p>
<pre class="python"><code>from gensim.models.phrases import Phrases, Phraser

# first train a bigram
bigram = Phrases(r.words)
trigram = Phrases(bigram[r.words])</code></pre>
<p>Using a portion of the testing data, we feed in a sentence and let the trigram connect the most likely bigrams together.</p>
<pre class="python"><code># print the original sentence
print(&quot;Original Sentence:&quot;)</code></pre>
<pre><code>## Original Sentence:</code></pre>
<pre class="python"><code>print(r.twords[86])

# run the sentence through the trigram model</code></pre>
<pre><code>## [&#39;this&#39;, &#39;rule&#39;, &#39;does&#39;, &#39;not&#39;, &#39;apply&#39;, &#39;to&#39;, &#39;a&#39;, &#39;transfer&#39;, &#39;to&#39;, &#39;an&#39;, &#39;individual&#39;, &#39;who&#39;, &#39;is&#39;, &#39;not&#39;, &#39;a&#39;, &#39;lineal&#39;, &#39;descendant&#39;, &#39;of&#39;, &#39;the&#39;, &#39;transferor&#39;, &#39;if&#39;, &#39;the&#39;, &#39;transferor&#39;, &#39;has&#39;, &#39;any&#39;, &#39;living&#39;, &#39;lineal&#39;, &#39;descendants&#39;]</code></pre>
<pre class="python"><code>print(f&quot;\n\nPhrased Sentence:&quot;)</code></pre>
<pre><code>## 
## 
## Phrased Sentence:</code></pre>
<pre class="python"><code>print(trigram[r.twords[86]])</code></pre>
<pre><code>## [&#39;this_rule&#39;, &#39;does_not&#39;, &#39;apply&#39;, &#39;to&#39;, &#39;a&#39;, &#39;transfer&#39;, &#39;to&#39;, &#39;an_individual&#39;, &#39;who&#39;, &#39;is&#39;, &#39;not&#39;, &#39;a&#39;, &#39;lineal_descendant&#39;, &#39;of&#39;, &#39;the&#39;, &#39;transferor&#39;, &#39;if&#39;, &#39;the&#39;, &#39;transferor&#39;, &#39;has&#39;, &#39;any&#39;, &#39;living&#39;, &#39;lineal&#39;, &#39;descendants&#39;]</code></pre>
<p>Note that we don’t see any trigrams in this example, but a quick look at the trigram model shows a plethora of 3 word tuples.</p>
