---
title: Phrases and Collocation Detection with Gensim
author: Alex Wold
date: '2021-06-24'
slug: phrases-and-collocation-detection-with-gensim
categories:
  - N-Grams
tags:
  - Gensim
  - Collocation
  - Phrases
  - N-Grams
subtitle: ''
summary: ''
authors: []
lastmod: '2021-06-24T01:17:10-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---
So far, we've used a set-list of replacement rules to incorporate n-grams within our tax-embeddings, but the Gensim Phrases library allows us to train a customized tokenizer to identify multiple-word terms specific to the IRS corpora using collocation detection. This pushes our model to capture expressions beyond the default unigram level and our own preservation strategies.

It's easier to work directly in python when using the Gensim functionalities. Therefore, we'll start with the following libraries and load a our example corpus for training and testing.
```{r}
# load packages
library(reticulate)
library(stringr)
library(tokenizers)

# load the instructions (as object instext)
textpath <- "/home/alex/Projects/taxembed/data/corpora/irs_corpi/instructions.rda"
load(textpath)
```
*instructions.rda* refers to a vector of strings, where each string is the ripped text from an individual IRS instruction posting. We don't need all of the data for a simple trial run, so we'll just use the first seven instruction texts (six for training, one for testing).

Unfortunately, the example corpus isn't very clean; regex expressions, like `\n` and `\t`, riddle the text with symbols that don't belong in the n-gram vocabulary. Luckily, the `stringr` package provides a few removal functions that can target these expressions and eliminate duplicate spaces. Afterwords, we can apply a sentence and word tokenizer to split the data into the form of a list, where each entry consists of a character vector of the separated words. We then convert the objects to their python equivalents, where we can access them by `r.words` for the training set and `r.twords` for the testing set.
```{r, results = 'hold'}
# create example corpus
excor <- paste(instext[1], instext[2], instext[3], instext[4], instext[5], instext[6])
testcor <- instext[7]

# replace regex characters and duplicate spaces
excor <- str_replace_all(excor, pattern = "(\\n|\\t)+", replacement = " ")
excor <- str_replace_all(excor, pattern = "\\s{2,}", replacement = " ")
testcor <- str_replace_all(testcor, pattern = "(\\n|\\t)+", replacement = " ")
testcor <- str_replace_all(testcor, pattern = "\\s{2,}", replacement = " ")

# tokenize the string into sentences
sentences <- unlist(tokenize_sentences(excor))
testsents <- unlist(tokenize_sentences(testcor))

# tokenize the sentences into words
words <- lapply(sentences, tokenize_words)
twords <- lapply(testsents, tokenize_words)

# unlist the secondary lists for proper form
for (i in 1:length(words)) {
  words[[i]] <- unlist(words[i])
}

for (i in 1:length(twords)) {
  twords[[i]] <- unlist(twords[i])
}

# convert the r object to a python object (# access with r.words in python chunk)
r_to_py(words)
r_to_py(twords)
```

It's easier to work directly in python when using the Gensim library, so we'll need to switch to an anaconda environment and load a few modules. Then, we train the first bigram and the succeeding trigram (by feeding the bigram through the first tgram).
```{python}
from gensim.models.phrases import Phrases, Phraser

# first train a bigram
bigram = Phrases(r.words)
trigram = Phrases(bigram[r.words])
```

Using a portion of the testing data, we feed in a sentence and let the trigram connect the most likely bigrams together.
```{python, warning = FALSE, message = FALSE, results = 'hold'}
# print the original sentence
print("Original Sentence:")
print(r.twords[86])

# run the sentence through the trigram model
print(f"\n\nPhrased Sentence:")
print(trigram[bigram[r.twords[86]]])
```
Note: we must run the sentence through the bigram layer first, and then through the subsequent, higher-level phrasers thereafter. Some vocabulary terms look like 4-grams, but they're just two bigrams attached in the trigram process.

In the training stage, we can further adjust the phraser using the tuning parameters available with the `Phrases()` function. *min_count* will ignore all words and bigrams with total collected counts lower than the provided value, and *threshold* will determine the sensitivity of the model when detecting n-grams. The parser will accept Phrase A followed by Phrase B as a bigram only if it scores higher than this threshold value, which in turn relies on the *scoring* parameter. Therefore, higher threshold values result in a lower amount of recognized bigrams, see the example below.
```{python, warning = FALSE, message = FALSE, results = 'hold'}
from gensim.models.phrases import Phrases, Phraser

# first train a bigram
bigram2 = Phrases(r.words, threshold = 50)
trigram2 = Phrases(bigram[r.words], threshold = 50)

# print the original sentence
print("Original Sentence:")
print(r.twords[86])

# run the sentence through the trigram model
print(f"\n\nPhrased Sentence:")
print(trigram2[bigram2[r.twords[86]]])
```
Note: "this" and "rule" were not concatenated as "this_rule" with the higher threshold. A higher value means fewer phrases.  
  
Again, this threshold depends on the scoring function used to specify how the model evaluates potential phrases. Gensim allows the user to change the default function to npmi_scorer(), or they can provide their own method.

^[https://radimrehurek.com/gensim/models/phrases.html]
^[https://radimrehurek.com/gensim_3.8.3/models/phrases.html]
^[https://tedboy.github.io/nlps/_modules/gensim/models/phrases.html]
^[https://cran.r-project.org/web/packages/tokenizers/vignettes/introduction-to-tokenizers.html]
