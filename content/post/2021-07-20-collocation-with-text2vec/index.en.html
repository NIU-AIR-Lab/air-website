---
title: "Collocation with text2vec"
author: Alex Wold
date: '2021-07-20'
slug: collocation-with-text2vec
categories:
  - N-Grams
  - text2vec
tags:
  - N-Grams
  - Collocation
  - text2vec
subtitle: ''
summary: ''
authors: []
lastmod: '2021-07-20T23:03:05-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<p>For this project, we’ll use the following libraries:</p>
<pre class="r"><code>library(text2vec)
library(stringr)
library(tokenizers)
library(doParallel)
library(parallel)</code></pre>
<p>Like with the Quanteda and Phrases suites, we want to take advantage of collocation statistics to extract n-grams from text data to better inform our downstream tasks. The <code>text2vec</code> package offers a concise, efficient, and flexible alternative to the aforementioned, see <a href="http://text2vec.org/collocations.html#example">their documentation</a>.</p>
<p>Here, we’ll prepare a small corpus from a few IRS instruction articles. Unfortunately, needless formatting characters still litter the text, so we’ll use the <code>stringr</code> package to remove the despoiling trammels and use <code>tokenizers</code> to split the data into sentences.</p>
<pre class="r"><code># load IRS instruction vector
textpath &lt;- &quot;/home/alex/Projects/taxembed/data/corpora/irs_corpi/instructions.rda&quot;
load(textpath)

# create an example corpus from the first several instruction texts
excor &lt;- paste(instext[1], instext[2], instext[3], instext[4], instext[5], instext[6])

# clean the text
excor &lt;- str_replace_all(excor, pattern = &quot;(\\n|\\t)+&quot;, replacement = &quot; &quot;)
excor &lt;- str_replace_all(excor, pattern = &quot;\\s{2,}&quot;, replacement = &quot; &quot;)

# tokenize the corpus into sentences
sentences &lt;- unlist(tokenize_sentences(excor))

head(sentences, 4)</code></pre>
<pre><code>## [1] &quot;Instructions for Form 56 (12/2019) Notice Concerning Fiduciary Relationship Section references are to the Internal Revenue Code unless otherwise noted.&quot;                                                                                                  
## [2] &quot;Revised: 12/2019 Instructions for Form 56 - Introductory Material Future Developments For the latest information about developments related to Form 56 and its instructions, such as legislation enacted after they were published, go to IRS.gov/Form56.&quot;
## [3] &quot;Photographs of Missing Children The IRS is a proud partner with the National Center for Missing &amp; Exploited Children® (NCMEC).&quot;                                                                                                                           
## [4] &quot;Photographs of missing children selected by the Center may appear in instructions on pages that would otherwise be blank.&quot;</code></pre>
<p>In order to utilize the <code>text2vec</code> phrase-building tools, we’ll need to call <code>itoken()</code> to create an iterator object that generates vocabularies or DTM (also TCM) matrices. This function requires either</p>
<ul>
<li>A List
<ul>
<li>all elements of the input list should be tokenized character vectors</li>
</ul></li>
<li>A Raw Character Text Source
<ul>
<li>which also requires a tokenizer function</li>
</ul></li>
<li>ifiles from files
<ul>
<li>which also requires a function to read the file and a tokenizer</li>
</ul></li>
<li>idir from a directory
<ul>
<li>which also requires a function to read in the files and a tokenizer</li>
</ul></li>
<li>ifiles_parallel from files in parallel <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></li>
</ul>
<p>We’ll supply <code>itoken()</code> with the first option. Note: the <code>text2vec</code> package sets <code>space_tokenizer()</code> as <code>itoken()'s</code> default tokenizer; therefore, we only need to pass in a list of sentences. After constructing the appropriate iterable, we can pass the token object to</p>
<pre class="r"><code># create a text2vec friendly iterable
tok &lt;- itoken(sentences, preprocessor = tolower)

# fit the n-gram phraser model
ngram &lt;- Collocations$new()
ngram$fit(tok, n_iter = 5)</code></pre>
<pre><code>## INFO  [05:30:54.017] iteration 1 - found 18 collocations 
## INFO  [05:30:54.235] iteration 2 - found 20 collocations 
## INFO  [05:30:54.410] iteration 3 - converged</code></pre>
<p>The <strong>n_iter</strong> parameter tells the Collocations model to iterate over the input iterable <strong>n_iter</strong> times, allowing us to control the size of the n-grams in the return, provided the algorithm doesn’t converge before it reaches <strong>n_iter</strong>.</p>
<p>The model stores learned collocations in the <strong>collocation_stat</strong> field. Notice that some of the associations aren’t very meaningful, but we can adjust the sensitivity by either modifying the results directly, calling prune, or through the following parameters</p>
<ul>
<li>collocation_count_min
<ul>
<li>the model will discard any collocation instances observed fewer times than this threshold</li>
</ul></li>
<li>pmi_min, gensim_min, lfmd_min, and llr_min
<ul>
<li>the model will discard any collocation instances that score less than the provided values for these variables</li>
</ul></li>
</ul>
<p>Below, we investigate the results of the ngram model and prune it using a pmi_min threshold.</p>
<pre class="r"><code># display the first 10 collocations
head(ngram$collocation_stat[, c(-3, -4, -5)], 10)

# prune the collocations
ngram$prune(pmi_min = 7)
head(ngram$collocation_stat[, c(-3, -4, -5)], 10)</code></pre>
<pre><code>##        prefix  suffix      pmi       lfmd     gensim       llr iter
##  1:     (text  field) 9.521021  -9.521021 132.487772  927.0521    1
##  2: specially  valued 9.030295 -10.258512  56.021250  727.5341    1
##  3:       -_-     -_- 8.796411 -10.257549  67.822542  735.5724    2
##  4:         -       - 7.715619  -8.888404 136.185890 1585.6994    1
##  5: surviving  spouse 7.504852 -10.902810  62.136298  745.4755    1
##  6: qualified    heir 7.055812 -11.276900  47.761133  744.2161    1
##  7:     gross estate. 6.911109 -12.591529   4.628899  445.1555    1
##  8:      more    than 6.819218 -12.469589  12.099078  467.3394    1
##  9:   treated      as 6.730982 -12.716694   6.012745  474.9286    1
## 10:      form     706 6.603136 -11.222064  44.949759  871.2867    1
##       prefix suffix      pmi       lfmd    gensim       llr iter
## 1:     (text field) 9.521021  -9.521021 132.48777  927.0521    1
## 2: specially valued 9.030295 -10.258512  56.02125  727.5341    1
## 3:       -_-    -_- 8.796411 -10.257549  67.82254  735.5724    2
## 4:         -      - 7.715619  -8.888404 136.18589 1585.6994    1
## 5: surviving spouse 7.504852 -10.902810  62.13630  745.4755    1
## 6: qualified   heir 7.055812 -11.276900  47.76113  744.2161    1</code></pre>
<p>We can also use parallel processing to create DTMS and TCMS, but we need to register the parallel backend first with the <code>doParallel</code> package and these iterables do not work with the Collocations fit function.</p>
<pre class="r"><code># detect the number of cores, create a makeCluster
ncores &lt;- (detectCores() - 1)
c1 &lt;- makeCluster(ncores)

# register the parallel backend
registerDoParallel(c1, ncores)

# create a text2vec friendly iterable
ptok &lt;- itoken_parallel(sentences, preprocessor = tolower)

# create a dtm
vectorizer &lt;- hash_vectorizer(ngram = c(1, 5))
dtm1 &lt;- create_dtm(ptok, vectorizer = vectorizer)

# stop the cluster
stopCluster(c1)</code></pre>
<p>Like Quanteda, <code>text2vec</code> doesn’t provide a direct way to substitute the new n-grams into the text; however, the package offers a panopoly of useful functions targeted at creating and evaluating word embeddings. They provide an analogy checker - similar in theory to Gensim’s <code>evaluate_word_analogies()</code> function, a similarity function, and GLOVE trainer.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://www.rdocumentation.org/packages/text2vec/versions/0.6/topics/itoken" class="uri">https://www.rdocumentation.org/packages/text2vec/versions/0.6/topics/itoken</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
